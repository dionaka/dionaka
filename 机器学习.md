## ***目录***
##

*dionaka.rovn.ink*

 - [NLP](#4)

 - [过拟合](#6)

 - [增强学习](#9)


## 4
## Q:什么是自然语言处理(NLP),举例说明实际中应用场景
### A:[NLP](https://developer.aliyun.com/article/1562176)
Natural Language Processing(NLP)
是人工智能领域的一个重要分支，旨在让计算机能够理解、理解和生成人类语言。NLP技术的核心是将自然语言转换为计算机可读的形式。

## 6
## Q:什么是过拟合，在机器学习中，我们如何应对过拟合问题？
### A:过拟合(Overfitting)
模型过于复杂地学习了训练集中的噪声、异常值或特定模式，导致对新样本的泛化能力下降。
### 处理方法
- 数据增强：对原始训练数据进行变换操作（如随机翻转、随机旋转、随机缩放、随机裁剪等）从而扩增训练数据的规模和多样性。
- 正则化：在损失函数中引入正则化项（如L1正则化、L2正则化等），用于限制模型参数的大小，从而减小模型的复杂度。
- 早停策略：在训练过程中，根据验证集的性能表现，在验证集上性能不再提升时提前停止训练。这可以避免模型在训练集上过拟合，通常是在验证集的误差曲线或准确率曲线的拐点处停止训练。
- 随机丢弃：在训练过程中，随机丢弃一些神经元的输出来减少神经元之间的依赖性，强制网络在训练时学习多个独立的子网络。
- 增加训练数据量：减少数据偶然性，如噪声、异常值等。
- 降低模型复杂度：通过减少模型的层数、节点数或卷积核的个数等方式降低模型的复杂度，从而降低过拟合的风险。
- 交叉验证：核心思想是利用有限的数据样本进行多次训练和验证，常见方法：留出法，K折交叉验证，留一法交叉验证，分层K折交叉验证，时间序列交叉验证，自助法交叉验证
- 合适的特征选择：选择对预测目标真正有影响的特征，减少不相关或冗余特征的影响
- 批标准化：在网络的每一层输入进行归一化，可以加速网络的收敛速度，降低模型在训练数据上的过拟合风险。
- 模型集成：通过组合多个不同的模型（如集成学习、模型融合等方式），可以提高模型的泛化能力，减少过拟合的风险。

## 9
## Q:什么是增强学习？请解释增强学习中的三个关键要素以及它们的作用。
### A:[wiki-强化学习](https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)

强化学习就是智能体在不断试错的过程中，通过接收环境的反馈(奖励或惩罚)来学习如何做出最优决策的过程。

- 智能体（Agent）：它是增强学习的核心，代表决策者或学习者。智能体通过观察环境的状态，并据此采取行动，从而与环境进行交互。智能体的目标是学习一个策略，使得长期累积的奖励最大化。

- 环境（Environment）：它是智能体交互的对象。环境会接收智能体采取的行动，并返回下一个状态以及一个奖励信号。奖励信号用于评估智能体采取的行动的好坏，指导智能体的学习过程。

- 奖励（Reward）：它是增强学习的驱动力。奖励信号是环境给智能体的反馈，用于告诉智能体某个行为是否值得。智能体的目标是最大化长期累积的奖励，因此奖励信号对智能体的学习过程至关重要。